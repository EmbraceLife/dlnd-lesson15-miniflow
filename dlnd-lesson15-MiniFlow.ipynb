{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From numpy to tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **miniflow**\n",
    "- build a small library called miniflow to use, instead of tensorflow\n",
    "- differentiable graphs: tensorflow abstraction for build networks\n",
    "- backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Graphs\n",
    "- define neuralnet in graphs\n",
    "- forward propagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    ">**Neural net in terms of graphs**\n",
    "- A neural network is a graph of mathematical functions such as **linear combinations** and activation functions\n",
    "- The graph consists of nodes, and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58375218_example-neural-network/example-neural-network.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2016/November/58375218_example-neural-network/example-neural-network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    ">**Forward propagation**\n",
    "- you know it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **2 steps to build a network**\n",
    "- Define the graph of nodes and edges.\n",
    "- Propagate values through the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miniflow architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Build subclass of Node**\n",
    "- input node class: don't calc, just hold values\n",
    "- add node class: do actual addition ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Forward pass needs two methods**\n",
    "- topological_sort\n",
    "- forward_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **topological sort**\n",
    "- you need to flatten the graph in such a way where all the input dependencies for each node are resolved before trying to run its calculation\n",
    "- no need to know the details of topological sort algo\n",
    "- `topological_sort()` takes in a `feed_dict`, which is how we initially set a value for an Input node. The `feed_dict` is represented by the Python dictionary data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/October/581037bb_topological-sort.001/topological-sort.001.jpeg\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2016/October/581037bb_topological-sort.001/topological-sort.001.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Learning and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **neural net improve accuracy of add over time**\n",
    "- need to implement linear combination first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5892978b_neuron/neuron.png\" width=\"300\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5892978b_neuron/neuron.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Linear transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5892a358_screen-shot-2016-10-21-at-15.43.05/screen-shot-2016-10-21-at-15.43.05.png\" width=\"200\" height=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=200, height=100, url='https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5892a358_screen-shot-2016-10-21-at-15.43.05/screen-shot-2016-10-21-at-15.43.05.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **scalar input, vector weights, vector output, vector bias**\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581f9571_newx/newx.png)\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581f9571_neww/neww.png)\n",
    "$$b = [b_1, b_2, b_3 ... b_k]$$\n",
    "\n",
    "> **vector input, matrix weights, vector output, vector bias**\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581f9570_newx-1n/newx-1n.png)\n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581f956f_neww-nk/neww-nk.png)\n",
    "$$b = [b_1, b_2, b_3 ... b_k]$$\n",
    "\n",
    "> **matrix input, matrix weights, vector output, vector bias**\n",
    "- but each input is processed row by row, not a whole matrix at once \n",
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/5820bdff_x-mn/x-mn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://d17h27t6h515a5.cloudfront.net/topher/2016/October/58114a6a_screen-shot-2016-10-26-at-19.28.34/screen-shot-2016-10-26-at-19.28.34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning rate\n",
    "- This is more of a guessing game than anything else but empirically values in the range 0.1 to 0.0001 work well. The range 0.001 to 0.0001 is popular, as 0.1 and 0.01 are sometimes too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final form of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 124.433\n",
      "Epoch: 2, Loss: 33.468\n",
      "Epoch: 3, Loss: 26.954\n",
      "Epoch: 4, Loss: 26.468\n",
      "Epoch: 5, Loss: 23.300\n",
      "Epoch: 6, Loss: 19.230\n",
      "Epoch: 7, Loss: 14.215\n",
      "Epoch: 8, Loss: 18.586\n",
      "Epoch: 9, Loss: 13.600\n",
      "Epoch: 10, Loss: 14.810\n",
      "Epoch: 11, Loss: 21.206\n",
      "Epoch: 12, Loss: 14.295\n",
      "Epoch: 13, Loss: 16.769\n",
      "Epoch: 14, Loss: 12.765\n",
      "Epoch: 15, Loss: 11.641\n",
      "Epoch: 16, Loss: 17.200\n",
      "Epoch: 17, Loss: 11.331\n",
      "Epoch: 18, Loss: 10.182\n",
      "Epoch: 19, Loss: 9.998\n",
      "Epoch: 20, Loss: 9.992\n",
      "Epoch: 21, Loss: 10.407\n",
      "Epoch: 22, Loss: 11.072\n",
      "Epoch: 23, Loss: 12.573\n",
      "Epoch: 24, Loss: 11.438\n",
      "Epoch: 25, Loss: 9.363\n",
      "Epoch: 26, Loss: 12.077\n",
      "Epoch: 27, Loss: 10.685\n",
      "Epoch: 28, Loss: 10.814\n",
      "Epoch: 29, Loss: 11.289\n",
      "Epoch: 30, Loss: 9.973\n",
      "Epoch: 31, Loss: 8.379\n",
      "Epoch: 32, Loss: 8.187\n",
      "Epoch: 33, Loss: 8.305\n",
      "Epoch: 34, Loss: 9.763\n",
      "Epoch: 35, Loss: 8.819\n",
      "Epoch: 36, Loss: 9.805\n",
      "Epoch: 37, Loss: 7.717\n",
      "Epoch: 38, Loss: 9.702\n",
      "Epoch: 39, Loss: 8.036\n",
      "Epoch: 40, Loss: 8.846\n",
      "Epoch: 41, Loss: 9.138\n",
      "Epoch: 42, Loss: 9.527\n",
      "Epoch: 43, Loss: 7.913\n",
      "Epoch: 44, Loss: 9.044\n",
      "Epoch: 45, Loss: 10.129\n",
      "Epoch: 46, Loss: 8.302\n",
      "Epoch: 47, Loss: 8.083\n",
      "Epoch: 48, Loss: 10.045\n",
      "Epoch: 49, Loss: 6.447\n",
      "Epoch: 50, Loss: 8.081\n",
      "Epoch: 51, Loss: 6.745\n",
      "Epoch: 52, Loss: 7.599\n",
      "Epoch: 53, Loss: 9.223\n",
      "Epoch: 54, Loss: 7.096\n",
      "Epoch: 55, Loss: 7.626\n",
      "Epoch: 56, Loss: 9.042\n",
      "Epoch: 57, Loss: 7.230\n",
      "Epoch: 58, Loss: 7.845\n",
      "Epoch: 59, Loss: 8.185\n",
      "Epoch: 60, Loss: 7.413\n",
      "Epoch: 61, Loss: 6.642\n",
      "Epoch: 62, Loss: 6.311\n",
      "Epoch: 63, Loss: 7.388\n",
      "Epoch: 64, Loss: 6.438\n",
      "Epoch: 65, Loss: 8.199\n",
      "Epoch: 66, Loss: 7.237\n",
      "Epoch: 67, Loss: 7.683\n",
      "Epoch: 68, Loss: 7.278\n",
      "Epoch: 69, Loss: 6.521\n",
      "Epoch: 70, Loss: 7.186\n",
      "Epoch: 71, Loss: 7.995\n",
      "Epoch: 72, Loss: 7.382\n",
      "Epoch: 73, Loss: 7.953\n",
      "Epoch: 74, Loss: 5.601\n",
      "Epoch: 75, Loss: 6.542\n",
      "Epoch: 76, Loss: 5.602\n",
      "Epoch: 77, Loss: 7.072\n",
      "Epoch: 78, Loss: 7.839\n",
      "Epoch: 79, Loss: 6.027\n",
      "Epoch: 80, Loss: 7.416\n",
      "Epoch: 81, Loss: 6.863\n",
      "Epoch: 82, Loss: 6.908\n",
      "Epoch: 83, Loss: 5.839\n",
      "Epoch: 84, Loss: 7.995\n",
      "Epoch: 85, Loss: 7.158\n",
      "Epoch: 86, Loss: 6.981\n",
      "Epoch: 87, Loss: 6.636\n",
      "Epoch: 88, Loss: 8.004\n",
      "Epoch: 89, Loss: 5.592\n",
      "Epoch: 90, Loss: 7.625\n",
      "Epoch: 91, Loss: 5.478\n",
      "Epoch: 92, Loss: 7.037\n",
      "Epoch: 93, Loss: 7.356\n",
      "Epoch: 94, Loss: 6.831\n",
      "Epoch: 95, Loss: 6.897\n",
      "Epoch: 96, Loss: 5.662\n",
      "Epoch: 97, Loss: 5.696\n",
      "Epoch: 98, Loss: 6.615\n",
      "Epoch: 99, Loss: 5.849\n",
      "Epoch: 100, Loss: 5.886\n",
      "Epoch: 101, Loss: 7.054\n",
      "Epoch: 102, Loss: 7.835\n",
      "Epoch: 103, Loss: 6.567\n",
      "Epoch: 104, Loss: 6.972\n",
      "Epoch: 105, Loss: 6.378\n",
      "Epoch: 106, Loss: 6.145\n",
      "Epoch: 107, Loss: 6.153\n",
      "Epoch: 108, Loss: 6.115\n",
      "Epoch: 109, Loss: 5.220\n",
      "Epoch: 110, Loss: 6.866\n",
      "Epoch: 111, Loss: 6.400\n",
      "Epoch: 112, Loss: 5.800\n",
      "Epoch: 113, Loss: 5.667\n",
      "Epoch: 114, Loss: 5.991\n",
      "Epoch: 115, Loss: 5.438\n",
      "Epoch: 116, Loss: 6.418\n",
      "Epoch: 117, Loss: 6.728\n",
      "Epoch: 118, Loss: 5.706\n",
      "Epoch: 119, Loss: 5.985\n",
      "Epoch: 120, Loss: 6.874\n",
      "Epoch: 121, Loss: 6.381\n",
      "Epoch: 122, Loss: 6.064\n",
      "Epoch: 123, Loss: 7.363\n",
      "Epoch: 124, Loss: 6.312\n",
      "Epoch: 125, Loss: 5.616\n",
      "Epoch: 126, Loss: 6.058\n",
      "Epoch: 127, Loss: 4.433\n",
      "Epoch: 128, Loss: 6.423\n",
      "Epoch: 129, Loss: 7.082\n",
      "Epoch: 130, Loss: 5.705\n",
      "Epoch: 131, Loss: 5.605\n",
      "Epoch: 132, Loss: 6.072\n",
      "Epoch: 133, Loss: 7.461\n",
      "Epoch: 134, Loss: 6.488\n",
      "Epoch: 135, Loss: 5.642\n",
      "Epoch: 136, Loss: 5.738\n",
      "Epoch: 137, Loss: 6.241\n",
      "Epoch: 138, Loss: 5.677\n",
      "Epoch: 139, Loss: 6.600\n",
      "Epoch: 140, Loss: 5.404\n",
      "Epoch: 141, Loss: 5.427\n",
      "Epoch: 142, Loss: 5.583\n",
      "Epoch: 143, Loss: 5.366\n",
      "Epoch: 144, Loss: 5.337\n",
      "Epoch: 145, Loss: 5.756\n",
      "Epoch: 146, Loss: 6.330\n",
      "Epoch: 147, Loss: 4.712\n",
      "Epoch: 148, Loss: 6.764\n",
      "Epoch: 149, Loss: 6.261\n",
      "Epoch: 150, Loss: 6.888\n",
      "Epoch: 151, Loss: 6.062\n",
      "Epoch: 152, Loss: 5.196\n",
      "Epoch: 153, Loss: 5.721\n",
      "Epoch: 154, Loss: 5.865\n",
      "Epoch: 155, Loss: 5.711\n",
      "Epoch: 156, Loss: 4.909\n",
      "Epoch: 157, Loss: 5.511\n",
      "Epoch: 158, Loss: 5.375\n",
      "Epoch: 159, Loss: 5.275\n",
      "Epoch: 160, Loss: 5.984\n",
      "Epoch: 161, Loss: 5.268\n",
      "Epoch: 162, Loss: 5.468\n",
      "Epoch: 163, Loss: 5.774\n",
      "Epoch: 164, Loss: 5.591\n",
      "Epoch: 165, Loss: 5.758\n",
      "Epoch: 166, Loss: 4.408\n",
      "Epoch: 167, Loss: 5.482\n",
      "Epoch: 168, Loss: 6.184\n",
      "Epoch: 169, Loss: 6.179\n",
      "Epoch: 170, Loss: 5.384\n",
      "Epoch: 171, Loss: 6.228\n",
      "Epoch: 172, Loss: 5.506\n",
      "Epoch: 173, Loss: 5.348\n",
      "Epoch: 174, Loss: 4.895\n",
      "Epoch: 175, Loss: 5.110\n",
      "Epoch: 176, Loss: 5.832\n",
      "Epoch: 177, Loss: 5.894\n",
      "Epoch: 178, Loss: 4.498\n",
      "Epoch: 179, Loss: 6.444\n",
      "Epoch: 180, Loss: 5.676\n",
      "Epoch: 181, Loss: 6.916\n",
      "Epoch: 182, Loss: 5.508\n",
      "Epoch: 183, Loss: 5.923\n",
      "Epoch: 184, Loss: 5.759\n",
      "Epoch: 185, Loss: 5.858\n",
      "Epoch: 186, Loss: 5.118\n",
      "Epoch: 187, Loss: 4.653\n",
      "Epoch: 188, Loss: 5.041\n",
      "Epoch: 189, Loss: 6.276\n",
      "Epoch: 190, Loss: 5.865\n",
      "Epoch: 191, Loss: 4.656\n",
      "Epoch: 192, Loss: 5.539\n",
      "Epoch: 193, Loss: 5.784\n",
      "Epoch: 194, Loss: 4.511\n",
      "Epoch: 195, Loss: 5.550\n",
      "Epoch: 196, Loss: 4.513\n",
      "Epoch: 197, Loss: 6.332\n",
      "Epoch: 198, Loss: 4.454\n",
      "Epoch: 199, Loss: 6.404\n",
      "Epoch: 200, Loss: 5.891\n",
      "Epoch: 201, Loss: 5.847\n",
      "Epoch: 202, Loss: 4.749\n",
      "Epoch: 203, Loss: 5.510\n",
      "Epoch: 204, Loss: 5.552\n",
      "Epoch: 205, Loss: 5.604\n",
      "Epoch: 206, Loss: 4.774\n",
      "Epoch: 207, Loss: 5.099\n",
      "Epoch: 208, Loss: 4.393\n",
      "Epoch: 209, Loss: 4.781\n",
      "Epoch: 210, Loss: 5.694\n",
      "Epoch: 211, Loss: 6.077\n",
      "Epoch: 212, Loss: 4.758\n",
      "Epoch: 213, Loss: 5.450\n",
      "Epoch: 214, Loss: 4.881\n",
      "Epoch: 215, Loss: 4.753\n",
      "Epoch: 216, Loss: 4.723\n",
      "Epoch: 217, Loss: 5.551\n",
      "Epoch: 218, Loss: 5.580\n",
      "Epoch: 219, Loss: 4.539\n",
      "Epoch: 220, Loss: 5.269\n",
      "Epoch: 221, Loss: 4.731\n",
      "Epoch: 222, Loss: 3.972\n",
      "Epoch: 223, Loss: 3.985\n",
      "Epoch: 224, Loss: 4.879\n",
      "Epoch: 225, Loss: 4.430\n",
      "Epoch: 226, Loss: 4.451\n",
      "Epoch: 227, Loss: 5.601\n",
      "Epoch: 228, Loss: 5.391\n",
      "Epoch: 229, Loss: 4.527\n",
      "Epoch: 230, Loss: 5.770\n",
      "Epoch: 231, Loss: 5.327\n",
      "Epoch: 232, Loss: 5.406\n",
      "Epoch: 233, Loss: 4.992\n",
      "Epoch: 234, Loss: 5.150\n",
      "Epoch: 235, Loss: 5.334\n",
      "Epoch: 236, Loss: 4.908\n",
      "Epoch: 237, Loss: 4.820\n",
      "Epoch: 238, Loss: 3.921\n",
      "Epoch: 239, Loss: 5.770\n",
      "Epoch: 240, Loss: 5.004\n",
      "Epoch: 241, Loss: 4.106\n",
      "Epoch: 242, Loss: 5.569\n",
      "Epoch: 243, Loss: 4.892\n",
      "Epoch: 244, Loss: 4.792\n",
      "Epoch: 245, Loss: 4.795\n",
      "Epoch: 246, Loss: 4.997\n",
      "Epoch: 247, Loss: 4.471\n",
      "Epoch: 248, Loss: 4.445\n",
      "Epoch: 249, Loss: 5.058\n",
      "Epoch: 250, Loss: 4.431\n",
      "Epoch: 251, Loss: 4.891\n",
      "Epoch: 252, Loss: 4.860\n",
      "Epoch: 253, Loss: 4.446\n",
      "Epoch: 254, Loss: 5.004\n",
      "Epoch: 255, Loss: 5.473\n",
      "Epoch: 256, Loss: 4.685\n",
      "Epoch: 257, Loss: 4.893\n",
      "Epoch: 258, Loss: 4.904\n",
      "Epoch: 259, Loss: 4.379\n",
      "Epoch: 260, Loss: 4.146\n",
      "Epoch: 261, Loss: 4.136\n",
      "Epoch: 262, Loss: 4.363\n",
      "Epoch: 263, Loss: 4.441\n",
      "Epoch: 264, Loss: 4.763\n",
      "Epoch: 265, Loss: 4.790\n",
      "Epoch: 266, Loss: 4.558\n",
      "Epoch: 267, Loss: 4.586\n",
      "Epoch: 268, Loss: 5.003\n",
      "Epoch: 269, Loss: 4.394\n",
      "Epoch: 270, Loss: 4.507\n",
      "Epoch: 271, Loss: 3.972\n",
      "Epoch: 272, Loss: 5.044\n",
      "Epoch: 273, Loss: 4.988\n",
      "Epoch: 274, Loss: 4.311\n",
      "Epoch: 275, Loss: 4.804\n",
      "Epoch: 276, Loss: 4.669\n",
      "Epoch: 277, Loss: 4.352\n",
      "Epoch: 278, Loss: 4.869\n",
      "Epoch: 279, Loss: 4.890\n",
      "Epoch: 280, Loss: 4.116\n",
      "Epoch: 281, Loss: 4.137\n",
      "Epoch: 282, Loss: 4.175\n",
      "Epoch: 283, Loss: 5.023\n",
      "Epoch: 284, Loss: 4.040\n",
      "Epoch: 285, Loss: 4.723\n",
      "Epoch: 286, Loss: 4.920\n",
      "Epoch: 287, Loss: 3.897\n",
      "Epoch: 288, Loss: 4.997\n",
      "Epoch: 289, Loss: 4.271\n",
      "Epoch: 290, Loss: 5.121\n",
      "Epoch: 291, Loss: 4.418\n",
      "Epoch: 292, Loss: 3.904\n",
      "Epoch: 293, Loss: 4.326\n",
      "Epoch: 294, Loss: 5.104\n",
      "Epoch: 295, Loss: 4.738\n",
      "Epoch: 296, Loss: 4.410\n",
      "Epoch: 297, Loss: 4.250\n",
      "Epoch: 298, Loss: 4.224\n",
      "Epoch: 299, Loss: 4.719\n",
      "Epoch: 300, Loss: 4.690\n",
      "Epoch: 301, Loss: 5.279\n",
      "Epoch: 302, Loss: 3.969\n",
      "Epoch: 303, Loss: 5.335\n",
      "Epoch: 304, Loss: 4.268\n",
      "Epoch: 305, Loss: 4.781\n",
      "Epoch: 306, Loss: 4.752\n",
      "Epoch: 307, Loss: 4.559\n",
      "Epoch: 308, Loss: 4.249\n",
      "Epoch: 309, Loss: 4.547\n",
      "Epoch: 310, Loss: 4.751\n",
      "Epoch: 311, Loss: 4.555\n",
      "Epoch: 312, Loss: 5.170\n",
      "Epoch: 313, Loss: 5.228\n",
      "Epoch: 314, Loss: 6.218\n",
      "Epoch: 315, Loss: 4.054\n",
      "Epoch: 316, Loss: 4.467\n",
      "Epoch: 317, Loss: 4.460\n",
      "Epoch: 318, Loss: 4.625\n",
      "Epoch: 319, Loss: 4.708\n",
      "Epoch: 320, Loss: 5.127\n",
      "Epoch: 321, Loss: 4.572\n",
      "Epoch: 322, Loss: 4.153\n",
      "Epoch: 323, Loss: 4.659\n",
      "Epoch: 324, Loss: 4.012\n",
      "Epoch: 325, Loss: 3.628\n",
      "Epoch: 326, Loss: 4.346\n",
      "Epoch: 327, Loss: 4.296\n",
      "Epoch: 328, Loss: 4.916\n",
      "Epoch: 329, Loss: 4.549\n",
      "Epoch: 330, Loss: 4.994\n",
      "Epoch: 331, Loss: 4.478\n",
      "Epoch: 332, Loss: 4.856\n",
      "Epoch: 333, Loss: 4.180\n",
      "Epoch: 334, Loss: 4.330\n",
      "Epoch: 335, Loss: 4.407\n",
      "Epoch: 336, Loss: 4.912\n",
      "Epoch: 337, Loss: 4.045\n",
      "Epoch: 338, Loss: 4.323\n",
      "Epoch: 339, Loss: 4.159\n",
      "Epoch: 340, Loss: 4.505\n",
      "Epoch: 341, Loss: 4.219\n",
      "Epoch: 342, Loss: 4.556\n",
      "Epoch: 343, Loss: 4.439\n",
      "Epoch: 344, Loss: 4.376\n",
      "Epoch: 345, Loss: 4.453\n",
      "Epoch: 346, Loss: 3.663\n",
      "Epoch: 347, Loss: 5.062\n",
      "Epoch: 348, Loss: 3.826\n",
      "Epoch: 349, Loss: 4.692\n",
      "Epoch: 350, Loss: 4.348\n",
      "Epoch: 351, Loss: 4.243\n",
      "Epoch: 352, Loss: 4.417\n",
      "Epoch: 353, Loss: 4.056\n",
      "Epoch: 354, Loss: 4.399\n",
      "Epoch: 355, Loss: 4.058\n",
      "Epoch: 356, Loss: 4.935\n",
      "Epoch: 357, Loss: 4.510\n",
      "Epoch: 358, Loss: 4.085\n",
      "Epoch: 359, Loss: 3.972\n",
      "Epoch: 360, Loss: 4.109\n",
      "Epoch: 361, Loss: 3.976\n",
      "Epoch: 362, Loss: 4.552\n",
      "Epoch: 363, Loss: 4.455\n",
      "Epoch: 364, Loss: 3.869\n",
      "Epoch: 365, Loss: 5.285\n",
      "Epoch: 366, Loss: 3.847\n",
      "Epoch: 367, Loss: 4.411\n",
      "Epoch: 368, Loss: 3.876\n",
      "Epoch: 369, Loss: 4.656\n",
      "Epoch: 370, Loss: 4.608\n",
      "Epoch: 371, Loss: 4.840\n",
      "Epoch: 372, Loss: 4.574\n",
      "Epoch: 373, Loss: 5.282\n",
      "Epoch: 374, Loss: 4.092\n",
      "Epoch: 375, Loss: 3.980\n",
      "Epoch: 376, Loss: 4.050\n",
      "Epoch: 377, Loss: 4.541\n",
      "Epoch: 378, Loss: 4.772\n",
      "Epoch: 379, Loss: 4.297\n",
      "Epoch: 380, Loss: 5.149\n",
      "Epoch: 381, Loss: 5.077\n",
      "Epoch: 382, Loss: 4.372\n",
      "Epoch: 383, Loss: 3.898\n",
      "Epoch: 384, Loss: 5.134\n",
      "Epoch: 385, Loss: 4.484\n",
      "Epoch: 386, Loss: 4.996\n",
      "Epoch: 387, Loss: 4.035\n",
      "Epoch: 388, Loss: 4.165\n",
      "Epoch: 389, Loss: 4.327\n",
      "Epoch: 390, Loss: 4.829\n",
      "Epoch: 391, Loss: 4.527\n",
      "Epoch: 392, Loss: 4.283\n",
      "Epoch: 393, Loss: 4.177\n",
      "Epoch: 394, Loss: 4.926\n",
      "Epoch: 395, Loss: 4.469\n",
      "Epoch: 396, Loss: 4.718\n",
      "Epoch: 397, Loss: 4.484\n",
      "Epoch: 398, Loss: 4.658\n",
      "Epoch: 399, Loss: 4.629\n",
      "Epoch: 400, Loss: 4.356\n",
      "Epoch: 401, Loss: 4.146\n",
      "Epoch: 402, Loss: 4.393\n",
      "Epoch: 403, Loss: 4.258\n",
      "Epoch: 404, Loss: 4.702\n",
      "Epoch: 405, Loss: 3.982\n",
      "Epoch: 406, Loss: 5.102\n",
      "Epoch: 407, Loss: 4.176\n",
      "Epoch: 408, Loss: 4.004\n",
      "Epoch: 409, Loss: 4.435\n",
      "Epoch: 410, Loss: 3.777\n",
      "Epoch: 411, Loss: 3.941\n",
      "Epoch: 412, Loss: 4.692\n",
      "Epoch: 413, Loss: 4.251\n",
      "Epoch: 414, Loss: 4.665\n",
      "Epoch: 415, Loss: 4.202\n",
      "Epoch: 416, Loss: 4.332\n",
      "Epoch: 417, Loss: 3.690\n",
      "Epoch: 418, Loss: 4.056\n",
      "Epoch: 419, Loss: 4.269\n",
      "Epoch: 420, Loss: 3.866\n",
      "Epoch: 421, Loss: 4.507\n",
      "Epoch: 422, Loss: 4.455\n",
      "Epoch: 423, Loss: 3.982\n",
      "Epoch: 424, Loss: 4.596\n",
      "Epoch: 425, Loss: 4.405\n",
      "Epoch: 426, Loss: 4.239\n",
      "Epoch: 427, Loss: 4.140\n",
      "Epoch: 428, Loss: 4.255\n",
      "Epoch: 429, Loss: 4.737\n",
      "Epoch: 430, Loss: 4.101\n",
      "Epoch: 431, Loss: 4.229\n",
      "Epoch: 432, Loss: 4.013\n",
      "Epoch: 433, Loss: 4.165\n",
      "Epoch: 434, Loss: 4.062\n",
      "Epoch: 435, Loss: 5.153\n",
      "Epoch: 436, Loss: 4.398\n",
      "Epoch: 437, Loss: 3.907\n",
      "Epoch: 438, Loss: 3.808\n",
      "Epoch: 439, Loss: 4.313\n",
      "Epoch: 440, Loss: 3.584\n",
      "Epoch: 441, Loss: 4.085\n",
      "Epoch: 442, Loss: 3.957\n",
      "Epoch: 443, Loss: 4.591\n",
      "Epoch: 444, Loss: 4.486\n",
      "Epoch: 445, Loss: 4.332\n",
      "Epoch: 446, Loss: 5.118\n",
      "Epoch: 447, Loss: 4.279\n",
      "Epoch: 448, Loss: 4.741\n",
      "Epoch: 449, Loss: 4.254\n",
      "Epoch: 450, Loss: 3.728\n",
      "Epoch: 451, Loss: 4.242\n",
      "Epoch: 452, Loss: 4.329\n",
      "Epoch: 453, Loss: 4.334\n",
      "Epoch: 454, Loss: 3.942\n",
      "Epoch: 455, Loss: 3.873\n",
      "Epoch: 456, Loss: 3.927\n",
      "Epoch: 457, Loss: 4.135\n",
      "Epoch: 458, Loss: 5.178\n",
      "Epoch: 459, Loss: 4.312\n",
      "Epoch: 460, Loss: 4.434\n",
      "Epoch: 461, Loss: 3.849\n",
      "Epoch: 462, Loss: 4.114\n",
      "Epoch: 463, Loss: 3.627\n",
      "Epoch: 464, Loss: 4.670\n",
      "Epoch: 465, Loss: 4.093\n",
      "Epoch: 466, Loss: 4.294\n",
      "Epoch: 467, Loss: 4.601\n",
      "Epoch: 468, Loss: 4.100\n",
      "Epoch: 469, Loss: 4.761\n",
      "Epoch: 470, Loss: 4.020\n",
      "Epoch: 471, Loss: 3.783\n",
      "Epoch: 472, Loss: 4.377\n",
      "Epoch: 473, Loss: 4.278\n",
      "Epoch: 474, Loss: 4.084\n",
      "Epoch: 475, Loss: 4.192\n",
      "Epoch: 476, Loss: 4.305\n",
      "Epoch: 477, Loss: 4.282\n",
      "Epoch: 478, Loss: 4.036\n",
      "Epoch: 479, Loss: 4.455\n",
      "Epoch: 480, Loss: 5.101\n",
      "Epoch: 481, Loss: 3.940\n",
      "Epoch: 482, Loss: 4.778\n",
      "Epoch: 483, Loss: 3.994\n",
      "Epoch: 484, Loss: 4.458\n",
      "Epoch: 485, Loss: 4.495\n",
      "Epoch: 486, Loss: 4.103\n",
      "Epoch: 487, Loss: 4.305\n",
      "Epoch: 488, Loss: 4.187\n",
      "Epoch: 489, Loss: 3.983\n",
      "Epoch: 490, Loss: 4.427\n",
      "Epoch: 491, Loss: 4.500\n",
      "Epoch: 492, Loss: 3.973\n",
      "Epoch: 493, Loss: 3.946\n",
      "Epoch: 494, Loss: 4.916\n",
      "Epoch: 495, Loss: 4.286\n",
      "Epoch: 496, Loss: 3.523\n",
      "Epoch: 497, Loss: 4.400\n",
      "Epoch: 498, Loss: 3.601\n",
      "Epoch: 499, Loss: 4.544\n",
      "Epoch: 500, Loss: 4.484\n",
      "Epoch: 501, Loss: 4.407\n",
      "Epoch: 502, Loss: 3.793\n",
      "Epoch: 503, Loss: 3.978\n",
      "Epoch: 504, Loss: 4.345\n",
      "Epoch: 505, Loss: 4.059\n",
      "Epoch: 506, Loss: 3.980\n",
      "Epoch: 507, Loss: 4.273\n",
      "Epoch: 508, Loss: 4.241\n",
      "Epoch: 509, Loss: 4.590\n",
      "Epoch: 510, Loss: 4.107\n",
      "Epoch: 511, Loss: 4.161\n",
      "Epoch: 512, Loss: 4.634\n",
      "Epoch: 513, Loss: 4.962\n",
      "Epoch: 514, Loss: 3.344\n",
      "Epoch: 515, Loss: 3.855\n",
      "Epoch: 516, Loss: 4.044\n",
      "Epoch: 517, Loss: 4.234\n",
      "Epoch: 518, Loss: 4.343\n",
      "Epoch: 519, Loss: 4.122\n",
      "Epoch: 520, Loss: 3.662\n",
      "Epoch: 521, Loss: 4.226\n",
      "Epoch: 522, Loss: 3.935\n",
      "Epoch: 523, Loss: 3.459\n",
      "Epoch: 524, Loss: 4.343\n",
      "Epoch: 525, Loss: 4.672\n",
      "Epoch: 526, Loss: 4.065\n",
      "Epoch: 527, Loss: 4.069\n",
      "Epoch: 528, Loss: 3.670\n",
      "Epoch: 529, Loss: 4.398\n",
      "Epoch: 530, Loss: 4.264\n",
      "Epoch: 531, Loss: 4.733\n",
      "Epoch: 532, Loss: 3.675\n",
      "Epoch: 533, Loss: 3.575\n",
      "Epoch: 534, Loss: 3.953\n",
      "Epoch: 535, Loss: 4.426\n",
      "Epoch: 536, Loss: 3.786\n",
      "Epoch: 537, Loss: 4.140\n",
      "Epoch: 538, Loss: 3.683\n",
      "Epoch: 539, Loss: 3.681\n",
      "Epoch: 540, Loss: 3.777\n",
      "Epoch: 541, Loss: 4.032\n",
      "Epoch: 542, Loss: 3.990\n",
      "Epoch: 543, Loss: 4.208\n",
      "Epoch: 544, Loss: 3.526\n",
      "Epoch: 545, Loss: 4.271\n",
      "Epoch: 546, Loss: 4.445\n",
      "Epoch: 547, Loss: 3.368\n",
      "Epoch: 548, Loss: 4.134\n",
      "Epoch: 549, Loss: 3.597\n",
      "Epoch: 550, Loss: 4.551\n",
      "Epoch: 551, Loss: 4.002\n",
      "Epoch: 552, Loss: 3.942\n",
      "Epoch: 553, Loss: 4.741\n",
      "Epoch: 554, Loss: 4.332\n",
      "Epoch: 555, Loss: 3.951\n",
      "Epoch: 556, Loss: 4.299\n",
      "Epoch: 557, Loss: 3.862\n",
      "Epoch: 558, Loss: 4.475\n",
      "Epoch: 559, Loss: 3.722\n",
      "Epoch: 560, Loss: 4.325\n",
      "Epoch: 561, Loss: 3.997\n",
      "Epoch: 562, Loss: 4.960\n",
      "Epoch: 563, Loss: 4.598\n",
      "Epoch: 564, Loss: 5.000\n",
      "Epoch: 565, Loss: 4.207\n",
      "Epoch: 566, Loss: 4.401\n",
      "Epoch: 567, Loss: 4.187\n",
      "Epoch: 568, Loss: 4.687\n",
      "Epoch: 569, Loss: 3.891\n",
      "Epoch: 570, Loss: 3.955\n",
      "Epoch: 571, Loss: 4.330\n",
      "Epoch: 572, Loss: 3.692\n",
      "Epoch: 573, Loss: 3.934\n",
      "Epoch: 574, Loss: 4.216\n",
      "Epoch: 575, Loss: 4.202\n",
      "Epoch: 576, Loss: 4.548\n",
      "Epoch: 577, Loss: 5.237\n",
      "Epoch: 578, Loss: 4.336\n",
      "Epoch: 579, Loss: 4.562\n",
      "Epoch: 580, Loss: 3.650\n",
      "Epoch: 581, Loss: 4.338\n",
      "Epoch: 582, Loss: 4.290\n",
      "Epoch: 583, Loss: 4.139\n",
      "Epoch: 584, Loss: 3.969\n",
      "Epoch: 585, Loss: 3.752\n",
      "Epoch: 586, Loss: 4.438\n",
      "Epoch: 587, Loss: 3.886\n",
      "Epoch: 588, Loss: 3.588\n",
      "Epoch: 589, Loss: 4.723\n",
      "Epoch: 590, Loss: 4.497\n",
      "Epoch: 591, Loss: 4.245\n",
      "Epoch: 592, Loss: 3.788\n",
      "Epoch: 593, Loss: 3.921\n",
      "Epoch: 594, Loss: 4.159\n",
      "Epoch: 595, Loss: 4.548\n",
      "Epoch: 596, Loss: 4.555\n",
      "Epoch: 597, Loss: 4.002\n",
      "Epoch: 598, Loss: 3.888\n",
      "Epoch: 599, Loss: 3.630\n",
      "Epoch: 600, Loss: 3.781\n",
      "Epoch: 601, Loss: 3.441\n",
      "Epoch: 602, Loss: 4.274\n",
      "Epoch: 603, Loss: 4.079\n",
      "Epoch: 604, Loss: 3.571\n",
      "Epoch: 605, Loss: 4.261\n",
      "Epoch: 606, Loss: 3.300\n",
      "Epoch: 607, Loss: 4.458\n",
      "Epoch: 608, Loss: 3.724\n",
      "Epoch: 609, Loss: 4.383\n",
      "Epoch: 610, Loss: 4.729\n",
      "Epoch: 611, Loss: 4.065\n",
      "Epoch: 612, Loss: 4.031\n",
      "Epoch: 613, Loss: 4.693\n",
      "Epoch: 614, Loss: 3.491\n",
      "Epoch: 615, Loss: 3.915\n",
      "Epoch: 616, Loss: 3.633\n",
      "Epoch: 617, Loss: 3.996\n",
      "Epoch: 618, Loss: 3.752\n",
      "Epoch: 619, Loss: 4.292\n",
      "Epoch: 620, Loss: 3.644\n",
      "Epoch: 621, Loss: 4.024\n",
      "Epoch: 622, Loss: 4.336\n",
      "Epoch: 623, Loss: 4.394\n",
      "Epoch: 624, Loss: 4.090\n",
      "Epoch: 625, Loss: 4.229\n",
      "Epoch: 626, Loss: 4.154\n",
      "Epoch: 627, Loss: 3.907\n",
      "Epoch: 628, Loss: 3.745\n",
      "Epoch: 629, Loss: 4.680\n",
      "Epoch: 630, Loss: 3.302\n",
      "Epoch: 631, Loss: 3.870\n",
      "Epoch: 632, Loss: 4.006\n",
      "Epoch: 633, Loss: 4.420\n",
      "Epoch: 634, Loss: 3.983\n",
      "Epoch: 635, Loss: 3.818\n",
      "Epoch: 636, Loss: 4.341\n",
      "Epoch: 637, Loss: 3.391\n",
      "Epoch: 638, Loss: 3.822\n",
      "Epoch: 639, Loss: 4.119\n",
      "Epoch: 640, Loss: 4.032\n",
      "Epoch: 641, Loss: 3.645\n",
      "Epoch: 642, Loss: 3.628\n",
      "Epoch: 643, Loss: 3.648\n",
      "Epoch: 644, Loss: 3.630\n",
      "Epoch: 645, Loss: 4.151\n",
      "Epoch: 646, Loss: 3.601\n",
      "Epoch: 647, Loss: 4.293\n",
      "Epoch: 648, Loss: 3.491\n",
      "Epoch: 649, Loss: 4.247\n",
      "Epoch: 650, Loss: 3.781\n",
      "Epoch: 651, Loss: 3.559\n",
      "Epoch: 652, Loss: 4.956\n",
      "Epoch: 653, Loss: 3.688\n",
      "Epoch: 654, Loss: 3.861\n",
      "Epoch: 655, Loss: 3.642\n",
      "Epoch: 656, Loss: 3.243\n",
      "Epoch: 657, Loss: 3.724\n",
      "Epoch: 658, Loss: 3.781\n",
      "Epoch: 659, Loss: 3.632\n",
      "Epoch: 660, Loss: 3.852\n",
      "Epoch: 661, Loss: 4.441\n",
      "Epoch: 662, Loss: 4.509\n",
      "Epoch: 663, Loss: 3.869\n",
      "Epoch: 664, Loss: 4.002\n",
      "Epoch: 665, Loss: 3.882\n",
      "Epoch: 666, Loss: 3.700\n",
      "Epoch: 667, Loss: 4.128\n",
      "Epoch: 668, Loss: 3.799\n",
      "Epoch: 669, Loss: 3.806\n",
      "Epoch: 670, Loss: 4.107\n",
      "Epoch: 671, Loss: 4.055\n",
      "Epoch: 672, Loss: 4.748\n",
      "Epoch: 673, Loss: 4.022\n",
      "Epoch: 674, Loss: 3.868\n",
      "Epoch: 675, Loss: 3.960\n",
      "Epoch: 676, Loss: 3.667\n",
      "Epoch: 677, Loss: 3.635\n",
      "Epoch: 678, Loss: 3.537\n",
      "Epoch: 679, Loss: 3.920\n",
      "Epoch: 680, Loss: 4.154\n",
      "Epoch: 681, Loss: 4.318\n",
      "Epoch: 682, Loss: 4.175\n",
      "Epoch: 683, Loss: 4.536\n",
      "Epoch: 684, Loss: 3.660\n",
      "Epoch: 685, Loss: 4.046\n",
      "Epoch: 686, Loss: 4.046\n",
      "Epoch: 687, Loss: 4.079\n",
      "Epoch: 688, Loss: 4.367\n",
      "Epoch: 689, Loss: 4.372\n",
      "Epoch: 690, Loss: 3.253\n",
      "Epoch: 691, Loss: 4.189\n",
      "Epoch: 692, Loss: 3.578\n",
      "Epoch: 693, Loss: 4.736\n",
      "Epoch: 694, Loss: 4.161\n",
      "Epoch: 695, Loss: 4.049\n",
      "Epoch: 696, Loss: 4.219\n",
      "Epoch: 697, Loss: 3.877\n",
      "Epoch: 698, Loss: 4.145\n",
      "Epoch: 699, Loss: 4.019\n",
      "Epoch: 700, Loss: 4.238\n",
      "Epoch: 701, Loss: 4.229\n",
      "Epoch: 702, Loss: 4.108\n",
      "Epoch: 703, Loss: 3.427\n",
      "Epoch: 704, Loss: 4.486\n",
      "Epoch: 705, Loss: 3.507\n",
      "Epoch: 706, Loss: 3.880\n",
      "Epoch: 707, Loss: 4.028\n",
      "Epoch: 708, Loss: 3.568\n",
      "Epoch: 709, Loss: 4.385\n",
      "Epoch: 710, Loss: 3.776\n",
      "Epoch: 711, Loss: 4.113\n",
      "Epoch: 712, Loss: 4.587\n",
      "Epoch: 713, Loss: 3.830\n",
      "Epoch: 714, Loss: 3.693\n",
      "Epoch: 715, Loss: 3.820\n",
      "Epoch: 716, Loss: 3.796\n",
      "Epoch: 717, Loss: 3.728\n",
      "Epoch: 718, Loss: 4.170\n",
      "Epoch: 719, Loss: 4.083\n",
      "Epoch: 720, Loss: 3.996\n",
      "Epoch: 721, Loss: 3.727\n",
      "Epoch: 722, Loss: 3.796\n",
      "Epoch: 723, Loss: 3.949\n",
      "Epoch: 724, Loss: 3.740\n",
      "Epoch: 725, Loss: 2.834\n",
      "Epoch: 726, Loss: 4.114\n",
      "Epoch: 727, Loss: 3.726\n",
      "Epoch: 728, Loss: 4.185\n",
      "Epoch: 729, Loss: 4.216\n",
      "Epoch: 730, Loss: 3.586\n",
      "Epoch: 731, Loss: 4.044\n",
      "Epoch: 732, Loss: 3.693\n",
      "Epoch: 733, Loss: 3.846\n",
      "Epoch: 734, Loss: 4.259\n",
      "Epoch: 735, Loss: 3.777\n",
      "Epoch: 736, Loss: 4.255\n",
      "Epoch: 737, Loss: 3.962\n",
      "Epoch: 738, Loss: 4.068\n",
      "Epoch: 739, Loss: 3.217\n",
      "Epoch: 740, Loss: 3.850\n",
      "Epoch: 741, Loss: 4.029\n",
      "Epoch: 742, Loss: 3.934\n",
      "Epoch: 743, Loss: 4.115\n",
      "Epoch: 744, Loss: 4.449\n",
      "Epoch: 745, Loss: 3.763\n",
      "Epoch: 746, Loss: 3.418\n",
      "Epoch: 747, Loss: 3.907\n",
      "Epoch: 748, Loss: 4.318\n",
      "Epoch: 749, Loss: 4.454\n",
      "Epoch: 750, Loss: 3.704\n",
      "Epoch: 751, Loss: 3.736\n",
      "Epoch: 752, Loss: 3.656\n",
      "Epoch: 753, Loss: 3.599\n",
      "Epoch: 754, Loss: 4.019\n",
      "Epoch: 755, Loss: 3.990\n",
      "Epoch: 756, Loss: 3.617\n",
      "Epoch: 757, Loss: 4.304\n",
      "Epoch: 758, Loss: 3.870\n",
      "Epoch: 759, Loss: 3.861\n",
      "Epoch: 760, Loss: 4.312\n",
      "Epoch: 761, Loss: 3.816\n",
      "Epoch: 762, Loss: 3.864\n",
      "Epoch: 763, Loss: 3.570\n",
      "Epoch: 764, Loss: 3.679\n",
      "Epoch: 765, Loss: 3.680\n",
      "Epoch: 766, Loss: 4.127\n",
      "Epoch: 767, Loss: 3.959\n",
      "Epoch: 768, Loss: 4.053\n",
      "Epoch: 769, Loss: 3.851\n",
      "Epoch: 770, Loss: 4.064\n",
      "Epoch: 771, Loss: 4.032\n",
      "Epoch: 772, Loss: 3.947\n",
      "Epoch: 773, Loss: 4.119\n",
      "Epoch: 774, Loss: 3.326\n",
      "Epoch: 775, Loss: 3.362\n",
      "Epoch: 776, Loss: 3.711\n",
      "Epoch: 777, Loss: 3.761\n",
      "Epoch: 778, Loss: 4.395\n",
      "Epoch: 779, Loss: 3.298\n",
      "Epoch: 780, Loss: 4.174\n",
      "Epoch: 781, Loss: 3.469\n",
      "Epoch: 782, Loss: 4.059\n",
      "Epoch: 783, Loss: 3.728\n",
      "Epoch: 784, Loss: 3.766\n",
      "Epoch: 785, Loss: 3.821\n",
      "Epoch: 786, Loss: 3.984\n",
      "Epoch: 787, Loss: 3.683\n",
      "Epoch: 788, Loss: 3.821\n",
      "Epoch: 789, Loss: 3.766\n",
      "Epoch: 790, Loss: 3.903\n",
      "Epoch: 791, Loss: 3.565\n",
      "Epoch: 792, Loss: 3.788\n",
      "Epoch: 793, Loss: 3.839\n",
      "Epoch: 794, Loss: 3.582\n",
      "Epoch: 795, Loss: 3.864\n",
      "Epoch: 796, Loss: 3.772\n",
      "Epoch: 797, Loss: 4.280\n",
      "Epoch: 798, Loss: 3.379\n",
      "Epoch: 799, Loss: 3.807\n",
      "Epoch: 800, Loss: 3.804\n",
      "Epoch: 801, Loss: 4.135\n",
      "Epoch: 802, Loss: 3.980\n",
      "Epoch: 803, Loss: 3.678\n",
      "Epoch: 804, Loss: 3.127\n",
      "Epoch: 805, Loss: 3.635\n",
      "Epoch: 806, Loss: 3.800\n",
      "Epoch: 807, Loss: 3.651\n",
      "Epoch: 808, Loss: 3.972\n",
      "Epoch: 809, Loss: 3.584\n",
      "Epoch: 810, Loss: 4.066\n",
      "Epoch: 811, Loss: 4.258\n",
      "Epoch: 812, Loss: 3.671\n",
      "Epoch: 813, Loss: 3.834\n",
      "Epoch: 814, Loss: 4.022\n",
      "Epoch: 815, Loss: 4.144\n",
      "Epoch: 816, Loss: 3.949\n",
      "Epoch: 817, Loss: 4.410\n",
      "Epoch: 818, Loss: 4.057\n",
      "Epoch: 819, Loss: 4.433\n",
      "Epoch: 820, Loss: 4.262\n",
      "Epoch: 821, Loss: 4.131\n",
      "Epoch: 822, Loss: 3.395\n",
      "Epoch: 823, Loss: 3.575\n",
      "Epoch: 824, Loss: 3.654\n",
      "Epoch: 825, Loss: 3.898\n",
      "Epoch: 826, Loss: 3.544\n",
      "Epoch: 827, Loss: 4.154\n",
      "Epoch: 828, Loss: 3.884\n",
      "Epoch: 829, Loss: 3.570\n",
      "Epoch: 830, Loss: 4.197\n",
      "Epoch: 831, Loss: 3.820\n",
      "Epoch: 832, Loss: 4.075\n",
      "Epoch: 833, Loss: 3.548\n",
      "Epoch: 834, Loss: 3.626\n",
      "Epoch: 835, Loss: 3.606\n",
      "Epoch: 836, Loss: 4.182\n",
      "Epoch: 837, Loss: 3.449\n",
      "Epoch: 838, Loss: 3.808\n",
      "Epoch: 839, Loss: 3.521\n",
      "Epoch: 840, Loss: 4.469\n",
      "Epoch: 841, Loss: 3.601\n",
      "Epoch: 842, Loss: 3.591\n",
      "Epoch: 843, Loss: 4.051\n",
      "Epoch: 844, Loss: 3.482\n",
      "Epoch: 845, Loss: 4.269\n",
      "Epoch: 846, Loss: 3.976\n",
      "Epoch: 847, Loss: 3.601\n",
      "Epoch: 848, Loss: 3.373\n",
      "Epoch: 849, Loss: 3.409\n",
      "Epoch: 850, Loss: 4.180\n",
      "Epoch: 851, Loss: 3.740\n",
      "Epoch: 852, Loss: 3.548\n",
      "Epoch: 853, Loss: 4.463\n",
      "Epoch: 854, Loss: 4.338\n",
      "Epoch: 855, Loss: 3.698\n",
      "Epoch: 856, Loss: 3.765\n",
      "Epoch: 857, Loss: 3.368\n",
      "Epoch: 858, Loss: 4.286\n",
      "Epoch: 859, Loss: 4.264\n",
      "Epoch: 860, Loss: 3.534\n",
      "Epoch: 861, Loss: 4.108\n",
      "Epoch: 862, Loss: 3.412\n",
      "Epoch: 863, Loss: 3.527\n",
      "Epoch: 864, Loss: 3.563\n",
      "Epoch: 865, Loss: 3.372\n",
      "Epoch: 866, Loss: 4.247\n",
      "Epoch: 867, Loss: 4.411\n",
      "Epoch: 868, Loss: 3.601\n",
      "Epoch: 869, Loss: 4.141\n",
      "Epoch: 870, Loss: 3.368\n",
      "Epoch: 871, Loss: 3.253\n",
      "Epoch: 872, Loss: 3.650\n",
      "Epoch: 873, Loss: 3.803\n",
      "Epoch: 874, Loss: 3.564\n",
      "Epoch: 875, Loss: 3.867\n",
      "Epoch: 876, Loss: 4.429\n",
      "Epoch: 877, Loss: 3.486\n",
      "Epoch: 878, Loss: 4.298\n",
      "Epoch: 879, Loss: 3.635\n",
      "Epoch: 880, Loss: 3.892\n",
      "Epoch: 881, Loss: 3.524\n",
      "Epoch: 882, Loss: 4.166\n",
      "Epoch: 883, Loss: 3.928\n",
      "Epoch: 884, Loss: 3.302\n",
      "Epoch: 885, Loss: 4.058\n",
      "Epoch: 886, Loss: 4.324\n",
      "Epoch: 887, Loss: 3.923\n",
      "Epoch: 888, Loss: 3.820\n",
      "Epoch: 889, Loss: 3.950\n",
      "Epoch: 890, Loss: 3.636\n",
      "Epoch: 891, Loss: 3.861\n",
      "Epoch: 892, Loss: 3.892\n",
      "Epoch: 893, Loss: 4.085\n",
      "Epoch: 894, Loss: 3.132\n",
      "Epoch: 895, Loss: 3.972\n",
      "Epoch: 896, Loss: 3.507\n",
      "Epoch: 897, Loss: 3.588\n",
      "Epoch: 898, Loss: 4.145\n",
      "Epoch: 899, Loss: 3.235\n",
      "Epoch: 900, Loss: 3.996\n",
      "Epoch: 901, Loss: 3.468\n",
      "Epoch: 902, Loss: 3.124\n",
      "Epoch: 903, Loss: 4.014\n",
      "Epoch: 904, Loss: 3.698\n",
      "Epoch: 905, Loss: 3.506\n",
      "Epoch: 906, Loss: 3.676\n",
      "Epoch: 907, Loss: 4.156\n",
      "Epoch: 908, Loss: 4.073\n",
      "Epoch: 909, Loss: 3.725\n",
      "Epoch: 910, Loss: 3.739\n",
      "Epoch: 911, Loss: 4.106\n",
      "Epoch: 912, Loss: 3.621\n",
      "Epoch: 913, Loss: 3.721\n",
      "Epoch: 914, Loss: 3.431\n",
      "Epoch: 915, Loss: 4.091\n",
      "Epoch: 916, Loss: 3.895\n",
      "Epoch: 917, Loss: 3.468\n",
      "Epoch: 918, Loss: 3.651\n",
      "Epoch: 919, Loss: 3.966\n",
      "Epoch: 920, Loss: 3.360\n",
      "Epoch: 921, Loss: 3.654\n",
      "Epoch: 922, Loss: 4.184\n",
      "Epoch: 923, Loss: 3.716\n",
      "Epoch: 924, Loss: 3.147\n",
      "Epoch: 925, Loss: 3.137\n",
      "Epoch: 926, Loss: 3.496\n",
      "Epoch: 927, Loss: 3.109\n",
      "Epoch: 928, Loss: 3.486\n",
      "Epoch: 929, Loss: 3.671\n",
      "Epoch: 930, Loss: 3.660\n",
      "Epoch: 931, Loss: 4.233\n",
      "Epoch: 932, Loss: 3.686\n",
      "Epoch: 933, Loss: 4.053\n",
      "Epoch: 934, Loss: 3.873\n",
      "Epoch: 935, Loss: 4.033\n",
      "Epoch: 936, Loss: 4.028\n",
      "Epoch: 937, Loss: 3.535\n",
      "Epoch: 938, Loss: 4.139\n",
      "Epoch: 939, Loss: 4.348\n",
      "Epoch: 940, Loss: 4.474\n",
      "Epoch: 941, Loss: 3.493\n",
      "Epoch: 942, Loss: 3.962\n",
      "Epoch: 943, Loss: 3.231\n",
      "Epoch: 944, Loss: 3.893\n",
      "Epoch: 945, Loss: 4.315\n",
      "Epoch: 946, Loss: 3.753\n",
      "Epoch: 947, Loss: 3.871\n",
      "Epoch: 948, Loss: 4.709\n",
      "Epoch: 949, Loss: 3.605\n",
      "Epoch: 950, Loss: 4.044\n",
      "Epoch: 951, Loss: 3.796\n",
      "Epoch: 952, Loss: 3.194\n",
      "Epoch: 953, Loss: 2.969\n",
      "Epoch: 954, Loss: 3.789\n",
      "Epoch: 955, Loss: 3.511\n",
      "Epoch: 956, Loss: 3.589\n",
      "Epoch: 957, Loss: 3.883\n",
      "Epoch: 958, Loss: 4.330\n",
      "Epoch: 959, Loss: 3.654\n",
      "Epoch: 960, Loss: 3.546\n",
      "Epoch: 961, Loss: 3.176\n",
      "Epoch: 962, Loss: 3.778\n",
      "Epoch: 963, Loss: 3.680\n",
      "Epoch: 964, Loss: 4.332\n",
      "Epoch: 965, Loss: 3.954\n",
      "Epoch: 966, Loss: 3.732\n",
      "Epoch: 967, Loss: 3.401\n",
      "Epoch: 968, Loss: 3.846\n",
      "Epoch: 969, Loss: 3.541\n",
      "Epoch: 970, Loss: 3.960\n",
      "Epoch: 971, Loss: 4.357\n",
      "Epoch: 972, Loss: 3.274\n",
      "Epoch: 973, Loss: 3.443\n",
      "Epoch: 974, Loss: 3.549\n",
      "Epoch: 975, Loss: 3.476\n",
      "Epoch: 976, Loss: 3.786\n",
      "Epoch: 977, Loss: 4.203\n",
      "Epoch: 978, Loss: 3.772\n",
      "Epoch: 979, Loss: 3.096\n",
      "Epoch: 980, Loss: 3.868\n",
      "Epoch: 981, Loss: 3.888\n",
      "Epoch: 982, Loss: 3.486\n",
      "Epoch: 983, Loss: 4.106\n",
      "Epoch: 984, Loss: 3.943\n",
      "Epoch: 985, Loss: 3.550\n",
      "Epoch: 986, Loss: 3.338\n",
      "Epoch: 987, Loss: 3.624\n",
      "Epoch: 988, Loss: 3.795\n",
      "Epoch: 989, Loss: 3.626\n",
      "Epoch: 990, Loss: 3.780\n",
      "Epoch: 991, Loss: 3.695\n",
      "Epoch: 992, Loss: 3.554\n",
      "Epoch: 993, Loss: 4.243\n",
      "Epoch: 994, Loss: 3.705\n",
      "Epoch: 995, Loss: 3.997\n",
      "Epoch: 996, Loss: 3.591\n",
      "Epoch: 997, Loss: 3.363\n",
      "Epoch: 998, Loss: 4.165\n",
      "Epoch: 999, Loss: 3.649\n",
      "Epoch: 1000, Loss: 3.825\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Have fun with the number of epochs!\n",
    "\n",
    "Be warned that if you increase them too much,\n",
    "the VM will time out :)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data by mean as 0, 1 as 1 standard deviation\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "# num of features or input nodes for training set\n",
    "n_features = X_.shape[1]\n",
    "\n",
    "# num of hidden nodes \n",
    "n_hidden = 10\n",
    "\n",
    "# initialize W1 dimension (num-input-nodes, num-hidden-nodes) from input layer to hidden layer\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "\n",
    "# set all b1 (bias for W1) to be 0, num of b1 == num of nodes on hidden_layer\n",
    "b1_ = np.zeros(n_hidden)\n",
    "\n",
    "# initialize W2, dim (num-hidden-nodes, num-output-nodes), so just 1 output node\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "\n",
    "# set bias from hidden layer to output layer to be 0s, num-bias == num-output-nodes\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "# initialize input nodes for X (training features) and y (training targets)\n",
    "# X is matrix, y is vector\n",
    "X, y = Input(), Input()\n",
    "\n",
    "# initialize input nodes for W1, W2, b1 and b2\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "# initialize a Linear node for from inputs to hidden layer\n",
    "l1 = Linear(X, W1, b1)\n",
    "\n",
    "# initialize a sigmoid node from hidden layer\n",
    "s1 = Sigmoid(l1)\n",
    "\n",
    "# initialize a Linear node as output node\n",
    "l2 = Linear(s1, W2, b2)\n",
    "\n",
    "# initialize a MSE node from output node \n",
    "cost = MSE(y, l2)\n",
    "\n",
    "# feed variable placeholders with values \n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# Total number of examples or data points for training \n",
    "m = X_.shape[0]\n",
    "\n",
    "# train 11 data points for each epoch\n",
    "batch_size = 11\n",
    "\n",
    "# how much batches in each epoch == how many times updates (if each batch_size update once)\n",
    "steps_per_epoch = m // batch_size # try 9 // 4 == 2 or 9 // 5 == 1\n",
    "\n",
    "# all nodes flattened into a graph\n",
    "graph = topological_sort(feed_dict)\n",
    "\n",
    "# group all weights and biases into a list \n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "# print total number of data points for training\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4: \n",
    "\n",
    "# for each epoch: \n",
    "for i in range(epochs):\n",
    "\n",
    "    # set loss to be 0\n",
    "    loss = 0\n",
    "    \n",
    "    # for each of many updates within an epoch\n",
    "    for j in range(steps_per_epoch):\n",
    "        \n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples, of size 11 or any number you set as batch_size\n",
    "        # or randomize batch_size of rows of features and rows of target values \n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        # calc forward propagation\n",
    "        # calc backward propagation\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        # trainables are list of weights and biases\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        # the last node value is loss \n",
    "        # add up loss in each iteration\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     4,
     60
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting miniflow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile miniflow.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes (the inbound_nodes=[] above) with edges are taken as inbound_nodes into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        \n",
    "        \n",
    "        # The eventual value of this node. Set by running the forward() method.\n",
    "        # when initialize this node, or right now, set value to be None\n",
    "        self.value = None\n",
    "        \n",
    "        \n",
    "        # initialize A list of nodes that this node outputs to\n",
    "        # set them to be empty list \n",
    "        self.outbound_nodes = []\n",
    "        \n",
    "        \n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        \n",
    "        \n",
    "        # Sets this node (we are creating at this moment) as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "            \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        \n",
    "        # first of all, make sure Input node is a Node class\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object. Yes, it is right\n",
    "        self.gradients = {self: 0}\n",
    "        \n",
    "        \n",
    "        # Weights and bias may be inputs, so you need to sum \n",
    "        # the gradient from output gradients.                      ######### yes, i see ##########\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self] ######## explained by computational derivation of graph ?\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            # for l_2 node, grad_cost is MSE's gradient\n",
    "            grad_cost = n.gradients[self]\n",
    "            \n",
    "            # Set the partial of the loss with respect to this node's inputs: ################################\n",
    "            # therefore, dL2/dX * grad_cost = sum_(W) * grad_cost = sum_(W*grad_cost) \n",
    "            # therefore,                    = dot(MSE's gradient, inbound_weights.T)\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # why grad_cost first, and weight.T second? \n",
    "            # this way = we can get dim (1, num_weights_or_inputs)\n",
    "            \n",
    "            \n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            # therefore, dL2/dW * grad_cost = sum_(X) * grad_cost = sum_(X*grad_cost) \n",
    "            # therefore,                    = dot(inbound_X.T, MSE's gradient)\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # why X.T first, grad_cost second? \n",
    "            # this way = we can get dim (num_weights, 1)\n",
    "            \n",
    "\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            # dl_2/db_2 * grad_cost = sum(1) * grad_cost = sum(grad_cost)\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "            # 1. take derivative of sigmoid with respect to l1 or first linear combination  ##################\n",
    "            # 2. add up is like computional graph for adding up to different routes of gradients ################\n",
    "            # multiple grade_cost = one of sigmoid's outbound_nodes' gradient, it is to inherit gradient from above #\n",
    "            # https://hyp.is/yPFpUPADEea5ebOmVQr3pw/colah.github.io/posts/2015-08-Backprop/\n",
    "            # 3. there is one gradient accumulation below, as there is only one input to sigmoid node ##############\n",
    "            # 4. because there is no summation in the gradient, this is not dot product, just simple multiplicatin\n",
    "            \n",
    "            \n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Initialize the gradients to 0.  Yes, I agree\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        \n",
    "        \n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            \n",
    "            # 1. take derivative of sigmoid with respect to l1 or first linear combination  ##################\n",
    "            # 2. add up is like computional graph for adding up to different routes of gradients ################\n",
    "            # multiple grade_cost = one of sigmoid's outbound_nodes' gradient, it is to inherit gradient from above #\n",
    "            # https://hyp.is/yPFpUPADEea5ebOmVQr3pw/colah.github.io/posts/2015-08-Backprop/\n",
    "            # 3. there is one gradient accumulation below, as there is only one input to sigmoid node ##############\n",
    "            # 4. because there is no summation in the gradient, this is not dot product, just simple multiplicatin\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\" ####################################\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \n",
    "        Everything is a node: \n",
    "        There are input nodes, linear combination node, activation function node, loss function node, \n",
    "        X, y, W, b are inputs nodes too\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "\n",
    "        # convert dim(3,) to dim(3,1), so \n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1) \n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        # set m to be num of data points\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost with respect to the second linear combination\n",
    "    \n",
    "        \"\"\"\n",
    "        # take derivative of MSE with respect to y\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff # for y ##########################\n",
    "        \n",
    "        # take derivative of MSE with respect to a or y_hat\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff # for a or y_hat ##################\n",
    "\n",
    "\n",
    "# Now, we have methods or functions, instead of classes        \n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "# forward and backward pass function \n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # graph contains all nodes (inputs, Linear, sigmoid, MSE) flattened and ordered\n",
    "    # let each node do forward and backward calculation\n",
    "    \n",
    "    # Forward pass\n",
    "    # start from the first node and do forward() on it\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    # start from the last node to the first node, and do backward() on it\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables [W1, b1, W2, b2]\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        \n",
    "        # calc gradient of weights or biases only\n",
    "        partial = t.gradients[t]\n",
    "        \n",
    "        # update weights or biases, it indicates partial derivative of MSE with respect to W2, W1, b1 and b2\n",
    "        # in order words, gradient of W1 will inherit gradient cost all the way of the chain from W1 to Cost\n",
    "        t.value -= learning_rate * partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ = np.array([[1,2,3,4,5], [3,2,3,4,5], [5,2,3,4,5], [7,2,3,4,5], [9,2,3,4,5]])\n",
    "y_ = np.array([[2,4,5,6,7], [4,4,5,6,7], [6,4,5,6,7], [8,4,5,6,7], [10,4,5,6,7]])\n",
    "batch_size = 3\n",
    "X_batch, y_batch = resample(X_, y_, n_samples=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5],\n",
       "       [3, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 4, 5, 6, 7],\n",
       "       [2, 4, 5, 6, 7],\n",
       "       [4, 4, 5, 6, 7]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\frac{\\partial{E}}{\\partial{\\hat{y}}} = \\frac{2}{2}\\sum_x(y_x-\\hat{y})$ (when take a number of data points at once) $= (y_x - \\hat{y})$ when dealing with a single data point \n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{l_2}} =  \\frac{2}{m}\\sum_x(y_x-l_2) $ (when take a number of data points at once) $=\\frac{2}{1} (y_x - l_2)$ when dealing with a single data point "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial{E}}{\\partial{\\hat{y}}} =  = \\frac{2}{2}\\sum_x(y_x-\\hat{y})$ (when take a number of data points at once) $= (y_x - \\hat{y})$ when dealing with a single data point \n",
    "\n",
    "$\\frac{\\partial{C}}{\\partial{l_2}} =  \\frac{2}{m}\\sum_x(y_x-l_2) $ (when take a number of data points at once) $=\\frac{2}{1} (y_x - l_2)$ when dealing with a single data point \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
